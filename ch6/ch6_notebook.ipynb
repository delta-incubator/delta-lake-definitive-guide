{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "38b60d30-a820-4125-b4d5-c20415e1b23b",
   "metadata": {},
   "source": [
    "# Chapter 6: Maintaining your Delta Lake\n",
    "> Exercises based on the nyc_taxi dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c31e57b3-1810-433e-91b1-47654ae9db38",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/04/24 23:27:19 WARN HiveConf: HiveConf of name hive.stats.jdbc.timeout does not exist\n",
      "23/04/24 23:27:19 WARN HiveConf: HiveConf of name hive.stats.retries.wait does not exist\n",
      "23/04/24 23:27:21 WARN ObjectStore: Version information not found in metastore. hive.metastore.schema.verification is not enabled so recording the schema version 2.3.0\n",
      "23/04/24 23:27:21 WARN ObjectStore: setMetaStoreSchemaVersion called but recording version is disabled: version = 2.3.0, comment = Set by MetaStore UNKNOWN@172.17.0.2\n",
      "23/04/24 23:27:21 WARN ObjectStore: Failed to get database default, returning NoSuchObjectException\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/04/24 23:27:25 WARN HiveExternalCatalog: Couldn't find corresponding Hive SerDe for data source provider delta. Persisting data source table `default`.`nyc_taxi` into Hive metastore in Spark SQL specific format, which is NOT compatible with Hive.\n",
      "23/04/24 23:27:25 WARN SessionState: METASTORE_FILTER_HOOK will be ignored, since hive.security.authorization.manager is set to instance of HiveAuthorizerFactory.\n",
      "23/04/24 23:27:25 WARN HiveConf: HiveConf of name hive.internal.ss.authz.settings.applied.marker does not exist\n",
      "23/04/24 23:27:25 WARN HiveConf: HiveConf of name hive.stats.jdbc.timeout does not exist\n",
      "23/04/24 23:27:25 WARN HiveConf: HiveConf of name hive.stats.retries.wait does not exist\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DataFrame[]"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.sql(\"\"\"\n",
    "     CREATE TABLE IF NOT EXISTS default.nyc_taxi (\n",
    "       VendorID BIGINT\n",
    "     ) USING DELTA\n",
    "     TBLPROPERTIES('delta.logRetentionDuration'='interval 7 days');\n",
    "   \"\"\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "aff8175c-3a5a-4beb-b2c7-dd0189aef47c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+---------+-----------+\n",
      "|namespace|tableName|isTemporary|\n",
      "+---------+---------+-----------+\n",
      "|  default| nyc_taxi|      false|\n",
      "+---------+---------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"show tables\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "56ac91e5-37fe-4772-95a4-634a67da7f4b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# should be zero on your first pass\n",
    "# this means that while the table exists, there are no data files associated with it\n",
    "len(spark.table(\"default.nyc_taxi\").inputFiles())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1aa0af8-06c6-40d4-b659-afc8d20c0467",
   "metadata": {},
   "source": [
    "## Start Populating the Table\n",
    "> The next three commands are used to show Schema Evolution and Validation with Delta Lake\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "de164e96-5ef7-4385-9e23-be8d6672cfa0",
   "metadata": {},
   "outputs": [
    {
     "ename": "AnalysisException",
     "evalue": "Table default.nyc_taxi already exists",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAnalysisException\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[8], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Populate the Table reading the Parquet nyc_taxi Data\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;66;03m# note: this will fail and that is okay\u001b[39;00m\n\u001b[0;32m----> 3\u001b[0m (\u001b[43mspark\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\n\u001b[1;32m      4\u001b[0m \u001b[43m      \u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mformat\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mparquet\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m      5\u001b[0m \u001b[43m      \u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m/opt/spark/data/dldg/datasets/nyc_taxi/*.parquet\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m      6\u001b[0m \u001b[43m      \u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwrite\u001b[49m\n\u001b[1;32m      7\u001b[0m \u001b[43m      \u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mformat\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mdelta\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m      8\u001b[0m \u001b[43m      \u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msaveAsTable\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mdefault.nyc_taxi\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m)\n",
      "File \u001b[0;32m/opt/spark/python/pyspark/sql/readwriter.py:1041\u001b[0m, in \u001b[0;36mDataFrameWriter.saveAsTable\u001b[0;34m(self, name, format, mode, partitionBy, **options)\u001b[0m\n\u001b[1;32m   1039\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mformat\u001b[39m \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   1040\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\u001b[38;5;28mformat\u001b[39m)\n\u001b[0;32m-> 1041\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jwrite\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msaveAsTable\u001b[49m\u001b[43m(\u001b[49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/spark/python/lib/py4j-0.10.9.5-src.zip/py4j/java_gateway.py:1321\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1315\u001b[0m command \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mCALL_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1316\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_header \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1317\u001b[0m     args_command \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1318\u001b[0m     proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART\n\u001b[1;32m   1320\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client\u001b[38;5;241m.\u001b[39msend_command(command)\n\u001b[0;32m-> 1321\u001b[0m return_value \u001b[38;5;241m=\u001b[39m \u001b[43mget_return_value\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1322\u001b[0m \u001b[43m    \u001b[49m\u001b[43manswer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgateway_client\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtarget_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1324\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n\u001b[1;32m   1325\u001b[0m     temp_arg\u001b[38;5;241m.\u001b[39m_detach()\n",
      "File \u001b[0;32m/opt/spark/python/pyspark/sql/utils.py:196\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    192\u001b[0m converted \u001b[38;5;241m=\u001b[39m convert_exception(e\u001b[38;5;241m.\u001b[39mjava_exception)\n\u001b[1;32m    193\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(converted, UnknownException):\n\u001b[1;32m    194\u001b[0m     \u001b[38;5;66;03m# Hide where the exception came from that shows a non-Pythonic\u001b[39;00m\n\u001b[1;32m    195\u001b[0m     \u001b[38;5;66;03m# JVM exception message.\u001b[39;00m\n\u001b[0;32m--> 196\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m converted \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    197\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    198\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m\n",
      "\u001b[0;31mAnalysisException\u001b[0m: Table default.nyc_taxi already exists"
     ]
    }
   ],
   "source": [
    "# Populate the Table reading the Parquet nyc_taxi Data\n",
    "# note: this will fail and that is okay\n",
    "(spark.read\n",
    "      .format(\"parquet\")\n",
    "      .load(\"/opt/spark/data/dldg/datasets/nyc_taxi/*.parquet\")\n",
    "      .write\n",
    "      .format(\"delta\")\n",
    "      .saveAsTable(\"default.nyc_taxi\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ba42d257-59e3-46aa-8f4e-dafb821a82f4",
   "metadata": {},
   "outputs": [
    {
     "ename": "AnalysisException",
     "evalue": "A schema mismatch detected when writing to the Delta table (Table ID: 44fe9670-3600-42ae-98ee-ea9c3d1717e3).\nTo enable schema migration using DataFrameWriter or DataStreamWriter, please set:\n'.option(\"mergeSchema\", \"true\")'.\nFor other operations, set the session configuration\nspark.databricks.delta.schema.autoMerge.enabled to \"true\". See the documentation\nspecific to the operation for details.\n\nTable schema:\nroot\n-- VendorID: long (nullable = true)\n\n\nData schema:\nroot\n-- VendorID: long (nullable = true)\n-- tpep_pickup_datetime: timestamp (nullable = true)\n-- tpep_dropoff_datetime: timestamp (nullable = true)\n-- passenger_count: double (nullable = true)\n-- trip_distance: double (nullable = true)\n-- RatecodeID: double (nullable = true)\n-- store_and_fwd_flag: string (nullable = true)\n-- PULocationID: long (nullable = true)\n-- DOLocationID: long (nullable = true)\n-- payment_type: long (nullable = true)\n-- fare_amount: double (nullable = true)\n-- extra: double (nullable = true)\n-- mta_tax: double (nullable = true)\n-- tip_amount: double (nullable = true)\n-- tolls_amount: double (nullable = true)\n-- improvement_surcharge: double (nullable = true)\n-- total_amount: double (nullable = true)\n-- congestion_surcharge: double (nullable = true)\n-- airport_fee: double (nullable = true)\n\n         ",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAnalysisException\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[10], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# one step closer, there is still something missing...\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;66;03m# and yes, this operation still fails... if only...\u001b[39;00m\n\u001b[0;32m----> 3\u001b[0m (\u001b[43mspark\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\n\u001b[1;32m      4\u001b[0m \u001b[43m      \u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mformat\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mparquet\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m      5\u001b[0m \u001b[43m      \u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m/opt/spark/data/dldg/datasets/nyc_taxi/*.parquet\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m      6\u001b[0m \u001b[43m      \u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwrite\u001b[49m\n\u001b[1;32m      7\u001b[0m \u001b[43m      \u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mformat\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mdelta\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m      8\u001b[0m \u001b[43m      \u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmode\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mappend\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m      9\u001b[0m \u001b[43m      \u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msaveAsTable\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mdefault.nyc_taxi\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m)\n",
      "File \u001b[0;32m/opt/spark/python/pyspark/sql/readwriter.py:1041\u001b[0m, in \u001b[0;36mDataFrameWriter.saveAsTable\u001b[0;34m(self, name, format, mode, partitionBy, **options)\u001b[0m\n\u001b[1;32m   1039\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mformat\u001b[39m \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   1040\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\u001b[38;5;28mformat\u001b[39m)\n\u001b[0;32m-> 1041\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jwrite\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msaveAsTable\u001b[49m\u001b[43m(\u001b[49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/spark/python/lib/py4j-0.10.9.5-src.zip/py4j/java_gateway.py:1321\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1315\u001b[0m command \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mCALL_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1316\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_header \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1317\u001b[0m     args_command \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1318\u001b[0m     proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART\n\u001b[1;32m   1320\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client\u001b[38;5;241m.\u001b[39msend_command(command)\n\u001b[0;32m-> 1321\u001b[0m return_value \u001b[38;5;241m=\u001b[39m \u001b[43mget_return_value\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1322\u001b[0m \u001b[43m    \u001b[49m\u001b[43manswer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgateway_client\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtarget_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1324\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n\u001b[1;32m   1325\u001b[0m     temp_arg\u001b[38;5;241m.\u001b[39m_detach()\n",
      "File \u001b[0;32m/opt/spark/python/pyspark/sql/utils.py:196\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    192\u001b[0m converted \u001b[38;5;241m=\u001b[39m convert_exception(e\u001b[38;5;241m.\u001b[39mjava_exception)\n\u001b[1;32m    193\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(converted, UnknownException):\n\u001b[1;32m    194\u001b[0m     \u001b[38;5;66;03m# Hide where the exception came from that shows a non-Pythonic\u001b[39;00m\n\u001b[1;32m    195\u001b[0m     \u001b[38;5;66;03m# JVM exception message.\u001b[39;00m\n\u001b[0;32m--> 196\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m converted \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    197\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    198\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m\n",
      "\u001b[0;31mAnalysisException\u001b[0m: A schema mismatch detected when writing to the Delta table (Table ID: 44fe9670-3600-42ae-98ee-ea9c3d1717e3).\nTo enable schema migration using DataFrameWriter or DataStreamWriter, please set:\n'.option(\"mergeSchema\", \"true\")'.\nFor other operations, set the session configuration\nspark.databricks.delta.schema.autoMerge.enabled to \"true\". See the documentation\nspecific to the operation for details.\n\nTable schema:\nroot\n-- VendorID: long (nullable = true)\n\n\nData schema:\nroot\n-- VendorID: long (nullable = true)\n-- tpep_pickup_datetime: timestamp (nullable = true)\n-- tpep_dropoff_datetime: timestamp (nullable = true)\n-- passenger_count: double (nullable = true)\n-- trip_distance: double (nullable = true)\n-- RatecodeID: double (nullable = true)\n-- store_and_fwd_flag: string (nullable = true)\n-- PULocationID: long (nullable = true)\n-- DOLocationID: long (nullable = true)\n-- payment_type: long (nullable = true)\n-- fare_amount: double (nullable = true)\n-- extra: double (nullable = true)\n-- mta_tax: double (nullable = true)\n-- tip_amount: double (nullable = true)\n-- tolls_amount: double (nullable = true)\n-- improvement_surcharge: double (nullable = true)\n-- total_amount: double (nullable = true)\n-- congestion_surcharge: double (nullable = true)\n-- airport_fee: double (nullable = true)\n\n         "
     ]
    }
   ],
   "source": [
    "# one step closer, there is still something missing...\n",
    "# and yes, this operation still fails... if only...\n",
    "(spark.read\n",
    "      .format(\"parquet\")\n",
    "      .load(\"/opt/spark/data/dldg/datasets/nyc_taxi/*.parquet\")\n",
    "      .write\n",
    "      .format(\"delta\")\n",
    "      .mode(\"append\")\n",
    "      .saveAsTable(\"default.nyc_taxi\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c83220eb-0589-4b95-b52e-c3d2892c5fd6",
   "metadata": {},
   "source": [
    "## Schema Evolution: Handle Automatically\n",
    "If you trust the upstream data source (provider) then you can add the `option(\"mergeSchema\", \"true\")`. Otherwise, it is better to specifically select a subset of the columns you expected to see. In this example use case, the only known column is the `VendorID`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6c202293-8be8-45e5-bb42-617fb57ee6dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Evolve the Schema. (Showcases how to auto-merge changes to the schema)\n",
    "# note: if you can trust the upstream, then this option is perfectly fine\n",
    "# however, if you don't trust the upstream, then it is good to opt-in to the \n",
    "# changing columns.\n",
    "\n",
    "(spark.read\n",
    "      .format(\"parquet\")\n",
    "      .load(\"/opt/spark/data/dldg/datasets/nyc_taxi/*.parquet\")\n",
    "      .write\n",
    "      .format(\"delta\")\n",
    "      .mode(\"append\")\n",
    "      .option(\"mergeSchema\", \"true\")\n",
    "      .saveAsTable(\"default.nyc_taxi\")\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "5ef0e41d-5a7b-4c4c-82a5-7e14d230204e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------------------+----------------------------------------------------------------------------------------------+-------+\n",
      "|col_name                    |data_type                                                                                     |comment|\n",
      "+----------------------------+----------------------------------------------------------------------------------------------+-------+\n",
      "|VendorID                    |bigint                                                                                        |       |\n",
      "|tpep_pickup_datetime        |timestamp                                                                                     |       |\n",
      "|tpep_dropoff_datetime       |timestamp                                                                                     |       |\n",
      "|passenger_count             |double                                                                                        |       |\n",
      "|trip_distance               |double                                                                                        |       |\n",
      "|RatecodeID                  |double                                                                                        |       |\n",
      "|store_and_fwd_flag          |string                                                                                        |       |\n",
      "|PULocationID                |bigint                                                                                        |       |\n",
      "|DOLocationID                |bigint                                                                                        |       |\n",
      "|payment_type                |bigint                                                                                        |       |\n",
      "|fare_amount                 |double                                                                                        |       |\n",
      "|extra                       |double                                                                                        |       |\n",
      "|mta_tax                     |double                                                                                        |       |\n",
      "|tip_amount                  |double                                                                                        |       |\n",
      "|tolls_amount                |double                                                                                        |       |\n",
      "|improvement_surcharge       |double                                                                                        |       |\n",
      "|total_amount                |double                                                                                        |       |\n",
      "|congestion_surcharge        |double                                                                                        |       |\n",
      "|airport_fee                 |double                                                                                        |       |\n",
      "|                            |                                                                                              |       |\n",
      "|# Partitioning              |                                                                                              |       |\n",
      "|Not partitioned             |                                                                                              |       |\n",
      "|                            |                                                                                              |       |\n",
      "|# Detailed Table Information|                                                                                              |       |\n",
      "|Name                        |default.nyc_taxi                                                                              |       |\n",
      "|Location                    |file:/opt/spark/work-dir/ch6/spark-warehouse/nyc_taxi                                         |       |\n",
      "|Provider                    |delta                                                                                         |       |\n",
      "|Owner                       |NBuser                                                                                        |       |\n",
      "|Table Properties            |[delta.logRetentionDuration=interval 7 days,delta.minReaderVersion=1,delta.minWriterVersion=2]|       |\n",
      "+----------------------------+----------------------------------------------------------------------------------------------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# view the table structure using DESCRIBE\n",
    "spark.sql(\"describe extended default.nyc_taxi\").show(30, truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "ffde40ae-604b-453c-bb25-4fb6592410f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+--------------------+---------------------+---------------+-------------+----------+------------------+------------+------------+------------+-----------+-----+-------+----------+------------+---------------------+------------+--------------------+-----------+\n",
      "|VendorID|tpep_pickup_datetime|tpep_dropoff_datetime|passenger_count|trip_distance|RatecodeID|store_and_fwd_flag|PULocationID|DOLocationID|payment_type|fare_amount|extra|mta_tax|tip_amount|tolls_amount|improvement_surcharge|total_amount|congestion_surcharge|airport_fee|\n",
      "+--------+--------------------+---------------------+---------------+-------------+----------+------------------+------------+------------+------------+-----------+-----+-------+----------+------------+---------------------+------------+--------------------+-----------+\n",
      "|       2| 2023-01-01 00:32:10|  2023-01-01 00:40:36|            1.0|         0.97|       1.0|                 N|         161|         141|           2|        9.3|  1.0|    0.5|       0.0|         0.0|                  1.0|        14.3|                 2.5|        0.0|\n",
      "|       2| 2023-01-01 00:55:08|  2023-01-01 01:01:27|            1.0|          1.1|       1.0|                 N|          43|         237|           1|        7.9|  1.0|    0.5|       4.0|         0.0|                  1.0|        16.9|                 2.5|        0.0|\n",
      "|       2| 2023-01-01 00:25:04|  2023-01-01 00:37:49|            1.0|         2.51|       1.0|                 N|          48|         238|           1|       14.9|  1.0|    0.5|      15.0|         0.0|                  1.0|        34.9|                 2.5|        0.0|\n",
      "|       1| 2023-01-01 00:03:48|  2023-01-01 00:13:25|            0.0|          1.9|       1.0|                 N|         138|           7|           1|       12.1| 7.25|    0.5|       0.0|         0.0|                  1.0|       20.85|                 0.0|       1.25|\n",
      "|       2| 2023-01-01 00:10:29|  2023-01-01 00:21:19|            1.0|         1.43|       1.0|                 N|         107|          79|           1|       11.4|  1.0|    0.5|      3.28|         0.0|                  1.0|       19.68|                 2.5|        0.0|\n",
      "|       2| 2023-01-01 00:50:34|  2023-01-01 01:02:52|            1.0|         1.84|       1.0|                 N|         161|         137|           1|       12.8|  1.0|    0.5|      10.0|         0.0|                  1.0|        27.8|                 2.5|        0.0|\n",
      "|       2| 2023-01-01 00:09:22|  2023-01-01 00:19:49|            1.0|         1.66|       1.0|                 N|         239|         143|           1|       12.1|  1.0|    0.5|      3.42|         0.0|                  1.0|       20.52|                 2.5|        0.0|\n",
      "|       2| 2023-01-01 00:27:12|  2023-01-01 00:49:56|            1.0|         11.7|       1.0|                 N|         142|         200|           1|       45.7|  1.0|    0.5|     10.74|         3.0|                  1.0|       64.44|                 2.5|        0.0|\n",
      "|       2| 2023-01-01 00:21:44|  2023-01-01 00:36:40|            1.0|         2.95|       1.0|                 N|         164|         236|           1|       17.7|  1.0|    0.5|      5.68|         0.0|                  1.0|       28.38|                 2.5|        0.0|\n",
      "|       2| 2023-01-01 00:39:42|  2023-01-01 00:50:36|            1.0|         3.01|       1.0|                 N|         141|         107|           2|       14.9|  1.0|    0.5|       0.0|         0.0|                  1.0|        19.9|                 2.5|        0.0|\n",
      "+--------+--------------------+---------------------+---------------+-------------+----------+------------------+------------+------------+------------+-----------+-----+-------+----------+------------+---------------------+------------+--------------------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"select * from default.nyc_taxi limit 10\").show(truncate=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4244ff8f-89e4-4aaf-8e5c-3363a58ec4f0",
   "metadata": {},
   "source": [
    "# Adding and Modifying Table Properties"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "28a5b346-e518-41bf-a16f-021772e020e8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.sql(\"\"\"\n",
    "ALTER TABLE default.nyc_taxi \n",
    "SET TBLPROPERTIES (\n",
    "  'delta.logRetentionDuration'='interval 14 days',\n",
    "  'delta.deletedFileRetentionDuration'='interval 28 days'\n",
    ")\"\"\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "184c2661-09e9-4798-a2e4-8181d5215213",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-----------------------+-----------------+\n",
      "|version|timestamp              |operation        |\n",
      "+-------+-----------------------+-----------------+\n",
      "|2      |2023-04-24 23:39:10.124|SET TBLPROPERTIES|\n",
      "|1      |2023-04-24 23:37:13.968|WRITE            |\n",
      "|0      |2023-04-24 23:27:22.46 |CREATE TABLE     |\n",
      "+-------+-----------------------+-----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# view the Table History\n",
    "# this will show the SET TBLPROPERTIES transaction in the Delta Lake transaction log\n",
    "from delta.tables import DeltaTable\n",
    "\n",
    "dt = DeltaTable.forName(spark, 'default.nyc_taxi')\n",
    "\n",
    "(\n",
    "    dt\n",
    "    .history(10)\n",
    "    .select(\"version\", \"timestamp\", \"operation\")\n",
    "    .show(truncate=False)\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "23d9a0fe-b95a-4f32-bf50-730b0b5207d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------------------------+----------------+\n",
      "|key                               |value           |\n",
      "+----------------------------------+----------------+\n",
      "|delta.deletedFileRetentionDuration|interval 28 days|\n",
      "|delta.logRetentionDuration        |interval 14 days|\n",
      "|delta.minReaderVersion            |1               |\n",
      "|delta.minWriterVersion            |2               |\n",
      "+----------------------------------+----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# view your Delta Lake tblproperties using Spark SQL\n",
    "# you'll see the tblproperties we added as well as the delta.minReaderVersion, delta.minWriterVersion\n",
    "spark.sql(\"show tblproperties default.nyc_taxi\").show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "be3d59ac-4ca3-4c36-9282-cd8446be8dfe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------------------------------------------------------------------------------------------+\n",
      "|properties                                                                                              |\n",
      "+--------------------------------------------------------------------------------------------------------+\n",
      "|{delta.logRetentionDuration -> interval 14 days, delta.deletedFileRetentionDuration -> interval 28 days}|\n",
      "+--------------------------------------------------------------------------------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# viewing the properties using the DeltaTable command\n",
    "(\n",
    "    DeltaTable\n",
    "    .forName(spark, 'default.nyc_taxi')\n",
    "    .detail()\n",
    "    .select(\"properties\")\n",
    "    .show(truncate=False)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "513ba79a-e127-4e3f-937d-20073155bb60",
   "metadata": {},
   "source": [
    "# Removing Table Properties"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "f4b84468-0f34-4996-8595-f84c2f1fc907",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# using unset to remove a property.\n",
    "# in the case of a key not existing, the operation becomes noop\n",
    "# so there is no need to use IF EXISTS conditionals\n",
    "spark.sql(\"\"\"\n",
    "    ALTER TABLE default.nyc_taxi\n",
    "    UNSET TBLPROPERTIES('delta.loRgetentionDuratio')\n",
    "  \"\"\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b42f760-7124-4be3-8a0b-0b2d3f5e4159",
   "metadata": {},
   "source": [
    "# Delta Table Optimizations (Cleaning, Tuning)\n",
    "> The following section showcases how to create and fix a poorly optimized Delta Lake table\n",
    "* Long Running Code will have a warning before hand."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "107377e1-990c-4ad5-ad28-a9d4d8abc3ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/04/24 23:48:30 WARN HiveExternalCatalog: Couldn't find corresponding Hive SerDe for data source provider delta. Persisting data source table `default`.`nonoptimal_nyc_taxi` into Hive metastore in Spark SQL specific format, which is NOT compatible with Hive.\n",
      "23/04/24 23:48:30 WARN SessionState: METASTORE_FILTER_HOOK will be ignored, since hive.security.authorization.manager is set to instance of HiveAuthorizerFactory.\n",
      "23/04/24 23:48:30 WARN HiveConf: HiveConf of name hive.internal.ss.authz.settings.applied.marker does not exist\n",
      "23/04/24 23:48:30 WARN HiveConf: HiveConf of name hive.stats.jdbc.timeout does not exist\n",
      "23/04/24 23:48:30 WARN HiveConf: HiveConf of name hive.stats.retries.wait does not exist\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<delta.tables.DeltaTable at 0xffff5fc1f940>"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# create a new table called `nonoptimal_nyc_taxi` in the `default` database.\n",
    "from delta.tables import DeltaTable\n",
    "(DeltaTable.createIfNotExists(spark)\n",
    "    .tableName(\"default.nonoptimal_nyc_taxi\")\n",
    "    .property(\"description\", \"table to be optimized\")\n",
    "    .addColumn(\"VendorID\", \"BIGINT\")\n",
    "    .addColumn(\"tpep_pickup_datetime\", \"TIMESTAMP\")\n",
    "    .addColumn(\"tpep_dropoff_datetime\", \"TIMESTAMP\")\n",
    "    .addColumn(\"passenger_count\", \"DOUBLE\")\n",
    "    .addColumn(\"trip_distance\", \"DOUBLE\")\n",
    "    .addColumn(\"RatecodeID\", \"DOUBLE\")\n",
    "    .addColumn(\"store_and_fwd_flag\", \"STRING\")\n",
    "    .addColumn(\"PULocationID\", \"BIGINT\")\n",
    "    .addColumn(\"DOLocationID\", \"BIGINT\")\n",
    "    .addColumn(\"payment_type\", \"BIGINT\")\n",
    "    .addColumn(\"fare_amount\", \"DOUBLE\")\n",
    "    .addColumn(\"extra\", \"DOUBLE\")\n",
    "    .addColumn(\"mta_tax\", \"DOUBLE\")\n",
    "    .addColumn(\"tip_amount\", \"DOUBLE\")\n",
    "    .addColumn(\"tolls_amount\", \"DOUBLE\")\n",
    "    .addColumn(\"improvement_surcharge\", \"DOUBLE\")\n",
    "    .addColumn(\"total_amount\", \"DOUBLE\")\n",
    "    .addColumn(\"congestion_surcharge\", \"DOUBLE\")\n",
    "    .addColumn(\"airport_fee\", \"DOUBLE\")\n",
    "    .execute()\n",
    "  )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "06a90c08-4c10-45a7-88b3-34b0c9567ab0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+-------------------+-----------+\n",
      "|namespace|          tableName|isTemporary|\n",
      "+---------+-------------------+-----------+\n",
      "|  default|nonoptimal_nyc_taxi|      false|\n",
      "|  default|           nyc_taxi|      false|\n",
      "+---------+-------------------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"show tables\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "9203909a-881e-4245-a205-c79d2bfa7dae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a function to take a Row and save as a separate Table\n",
    "# note: this is for example, this is not optimal\n",
    "# note 2: if you want to use something similar, please use `rows` to write a collection of rows per transaction\n",
    "def append_row_to_table(row, schema, table):\n",
    "    (spark.createDataFrame([row], schema)\n",
    "      .write\n",
    "      .format(\"delta\")\n",
    "      .mode(\"append\")\n",
    "      .saveAsTable(table))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "b4791275-2ce9-4e45-a010-fb1fceae477a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "####### Warning: will run for a while\n",
    "source_df = spark.table(\"default.nyc_taxi\")\n",
    "source_schema = source_df.schema\n",
    "destination_table = \"default.nonoptimal_nyc_taxi\"\n",
    "# change limit to 1000 or 10000 if you want to generate a more verbose example\n",
    "limit = 100\n",
    "\n",
    "# warning: list comprehension is used here to run synchronous inserts and to prove a point\n",
    "# please don't use this code for production use cases, and just to create a poorly optimized table\n",
    "([\n",
    "    append_row_to_table(row, source_schema, destination_table) \n",
    "    for row in source_df.limit(limit).collect()\n",
    "])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47d58725-bd63-4860-9470-309ca5f533c2",
   "metadata": {},
   "source": [
    "## Using Optimize\n",
    "> Using Bin-Packing Optimize (default) will allow us to coalesce many small files (which we just created) into fewer large files.\n",
    "\n",
    "Spark Config:\n",
    "1. `spark.databricks.delta.optimize.repartition.enabled=true` is useful when we have many small files like we do in the case of the nonoptimal_nyc_taxi Delta Lake Table.\n",
    "\n",
    "Databricks-Only: Delta Lake Table Properties:\n",
    "1. `delta.targetFileSize=20mb`\n",
    "2. `delta.tuneFileSizesForRewrites=true`\n",
    "\n",
    "When running OPTIMIZE outside of databricks, like we are inside this jupyter notebook, we can lean on some alternative spark configuration to control how many files we read and how we can optimize differently."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "7fca41c3-f010-4110-ac23-9885ca7e2229",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You are setting a property: delta.targetFileSize that is not recognized by this version of Delta\n",
      "You are setting a property: delta.tuneFileSizesForRewrites that is not recognized by this version of Delta\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DataFrame[]"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# modify the table properties for OPTIMIZE\n",
    "\n",
    "# note: these configurations are only for databricks at the time of writing so the \n",
    "# `spark.databricks.delta.allowArbitraryProperties.enabled` is used to prevent an exception from being thrown\n",
    "spark.conf.set('spark.databricks.delta.allowArbitraryProperties.enabled','true')\n",
    "spark.conf.set('spark.databricks.delta.optimize.repartition.enabled', 'true')\n",
    "spark.conf.set('spark.sql.files.maxRecordsPerFile', '1000000')\n",
    "spark.sql(\"\"\"\n",
    "    ALTER TABLE default.nonoptimal_nyc_taxi\n",
    "    SET TBLPROPERTIES (\n",
    "      'delta.targetFileSize'='20mb',\n",
    "      'delta.tuneFileSizesForRewrites'='true'\n",
    "    )\n",
    "  \"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "7b390c1d-a8f4-4189-9e12-c541dc758cce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+\n",
      "|                path|             metrics|\n",
      "+--------------------+--------------------+\n",
      "|file:/opt/spark/w...|{0, 0, {null, nul...|\n",
      "+--------------------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# execute OPTIMIZE from the Delta python client\n",
    "\n",
    "df = (\n",
    "    DeltaTable.forName(spark, \"default.nonoptimal_nyc_taxi\")\n",
    "    .optimize()\n",
    "    .executeCompaction()\n",
    ")\n",
    "df.show(truncate=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "742d2d9b-4719-4b39-8382-6e784ce8bb6b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-----------------------+---------+---------------+-------------+\n",
      "|version|timestamp              |operation|numRemovedFiles|numAddedFiles|\n",
      "+-------+-----------------------+---------+---------------+-------------+\n",
      "|103    |2023-04-25 00:08:11.166|OPTIMIZE |200            |1            |\n",
      "+-------+-----------------------+---------+---------------+-------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# view the full metadata from the OPTIMIZE operation\n",
    "from pyspark.sql.functions import col\n",
    "(\n",
    "    DeltaTable\n",
    "    .forName(spark, \"default.nonoptimal_nyc_taxi\")\n",
    "    .history(2)\n",
    "    .where(col(\"operation\") == \"OPTIMIZE\")\n",
    "    .select(\"version\", \"timestamp\", \"operation\", \"operationMetrics.numRemovedFiles\", \"operationMetrics.numAddedFiles\")\n",
    "    .show(truncate=False)\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42434366-d128-46be-b579-c866bdde76a6",
   "metadata": {},
   "source": [
    "# Using Z-Order Optimize\n",
    "> Z-Order optimize is a co-location technique to minimize the total number of files loaded to answer common questions (queries) from your Delta Lake tables. For example, letâ€™s say 80% of the queries to the `nyc_taxi dataset` always search first for `tpep_pickup_datetime` followed by a specific `RatecodeID`. You could optimize for faster query results by co-locating the `tpep_pickup_datetime` and `RatecodeID` so that the search space is reduced\n",
    "> This allows us to reduce the number of files that need to be opened, using data skipping, since Delta Lake captures statistics automatically for the first 32 columns of a Delta Lake table.\n",
    "\n",
    "## Delta Lake Table Properties\n",
    "`delta.dataSkippingNumIndexedCols=6` could be used in the case where we only care about the first 6 columns of our Delta Lake table.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "ebfcbd0c-5ebb-4d68-9d99-a76804183a38",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[]"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# change the statistics collected for the table from the default 32 down to 6\n",
    "# since we will be calling zorder optimize, this setting can take effect\n",
    "# along with the operation itself\n",
    "spark.sql(\"\"\"\n",
    "    ALTER TABLE default.nonoptimal_nyc_taxi\n",
    "    SET TBLPROPERTIES (\n",
    "      'delta.dataSkippingNumIndexedCols'='6'\n",
    "    )\n",
    "  \"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "05de0efe-917b-47a1-82bf-3b5dc5498fa3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[path: string, metrics: struct<numFilesAdded:bigint,numFilesRemoved:bigint,filesAdded:struct<min:bigint,max:bigint,avg:double,totalFiles:bigint,totalSize:bigint>,filesRemoved:struct<min:bigint,max:bigint,avg:double,totalFiles:bigint,totalSize:bigint>,partitionsOptimized:bigint,zOrderStats:struct<strategyName:string,inputCubeFiles:struct<num:bigint,size:bigint>,inputOtherFiles:struct<num:bigint,size:bigint>,inputNumCubes:bigint,mergedFiles:struct<num:bigint,size:bigint>,numOutputCubes:bigint,mergedNumCubes:bigint>,numBatches:bigint,totalConsideredFiles:bigint,totalFilesSkipped:bigint,preserveInsertionOrder:boolean,numFilesSkippedToReduceWriteAmplification:bigint,numBytesSkippedToReduceWriteAmplification:bigint,startTimeMs:bigint,endTimeMs:bigint,totalClusterParallelism:bigint,totalScheduledTasks:bigint,autoCompactParallelismStats:struct<maxClusterActiveParallelism:bigint,minClusterActiveParallelism:bigint,maxSessionActiveParallelism:bigint,minSessionActiveParallelism:bigint>>]"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# see https://docs.databricks.com/delta/data-skipping.html for more details\n",
    "dt = DeltaTable.forName(spark, \"default.nonoptimal_nyc_taxi\")\n",
    "(\n",
    "    dt\n",
    "    .optimize()\n",
    "    .executeZOrderBy(\"tpep_pickup_datetime\", \"RatecodeID\")\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "470ded9a-328b-4d6f-83ab-b6eecaedbdd7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-----------------------+---------+---------------+-------------+\n",
      "|version|timestamp              |operation|numRemovedFiles|numAddedFiles|\n",
      "+-------+-----------------------+---------+---------------+-------------+\n",
      "|107    |2023-04-25 01:01:56.656|OPTIMIZE |1              |1            |\n",
      "|106    |2023-04-25 00:55:36.921|OPTIMIZE |1              |1            |\n",
      "|103    |2023-04-25 00:08:11.166|OPTIMIZE |200            |1            |\n",
      "+-------+-----------------------+---------+---------------+-------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# view the results of the optimization\n",
    "# in the case where we user z-order optimize on a single file, it isn't going to help much\n",
    "# but you get the idea!\n",
    "(\n",
    "    dt.history(10)\n",
    "    .where(col(\"operation\") == \"OPTIMIZE\")\n",
    "    .select(\"version\", \"timestamp\", \"operation\", \"operationMetrics.numRemovedFiles\", \"operationMetrics.numAddedFiles\")\n",
    "    .show(truncate=False)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a628fd02-b940-49fa-8992-a2a3acd21c17",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
